{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function\n",
    "\n",
    "import random\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "\n",
    "\n",
    "\n",
    "first_names = [\"Donna\", \"donna\", \"Mandy\", \"mandy\", \"Colin\", \"colin\", \"Janis\", \"janis\", \"Brian\", \"brian\"]\n",
    "full_names = [\"Donna Graves\", \"donna graves\", \"Mandy Green\", \"mandy green\", \"Colin Banger\", \"colin banger\", \n",
    "              \"Janis Michael\", \"janis michael\", \"Brian Elliot\", \"brian elliot\"]\n",
    "\n",
    "TRAIN_DATA = []\n",
    "\n",
    "for name in first_names:\n",
    "    TRAIN_DATA.append((\"Find me {}'s office\".format(name), \n",
    "              { \"heads\": [0, 0, 4, 2, 0],  # index of token head\n",
    "                \"deps\": [\"ROOT\", \"-\", \"POSS\", \"-\", \"PLACE\"]}))\n",
    "    TRAIN_DATA.append((\"Could you find me {}'s office\".format(name), \n",
    "              { \"heads\": [2, 2, 2, 2, 6, 4, 2],  # index of token head\n",
    "                \"deps\": [\"-\", \"-\", \"ROOT\", \"-\", \"POSS\", \"-\", \"PLACE\"]}))\n",
    "    TRAIN_DATA.append((\"I want to find {}'s office\".format(name), \n",
    "              { \"heads\": [1, 1, 3, 1, 6, 4, 3],  # index of token head\n",
    "                \"deps\": ['-', 'ROOT', '-', 'XCOMP', 'POSS', '-', 'PLACE']}))\n",
    "    TRAIN_DATA.append((\"Where is {}'s office?\".format(name), \n",
    "              { \"heads\": [1, 1, 4, 2, 1, 1],  # index of token head\n",
    "                \"deps\": [\"-\", \"ROOT\", \"POSS\", \"-\", \"PLACE\", \"-\"]}))\n",
    "    TRAIN_DATA.append((\"Where can I find {}'s office?\".format(name), \n",
    "              { \"heads\": [3, 3, 3, 3, 6, 4, 3, 3],  # index of token head\n",
    "                \"deps\": [\"-\", \"-\", \"-\", \"ROOT\", \"POSS\", \"-\", \"PLACE\", \"-\"]}))\n",
    "    TRAIN_DATA.append((\"Let me know {}'s office\".format(name), \n",
    "              { \"heads\": [2, 2, 2, 5, 3, 2],  # index of token head\n",
    "                \"deps\": [\"-\", \"-\", \"ROOT\", \"POSS\", \"-\", \"PLACE\"]}))\n",
    "\n",
    "for name in full_names:\n",
    "    TRAIN_DATA.append((\"Find me {}'s office\".format(name), \n",
    "              { \"heads\": [0, 0, 3, 5, 3, 0],  # index of token head\n",
    "                \"deps\": [\"ROOT\", \"-\", \"POSS\", \"POSS\", \"-\", \"PLACE\"]}))\n",
    "    TRAIN_DATA.append((\"Could you find me {}'s office\".format(name), \n",
    "              { \"heads\": [2, 2, 2, 2, 6, 7, 6, 2],  # index of token head\n",
    "                \"deps\": [\"-\", \"-\", \"ROOT\", \"-\", \"POSS\", \"POSS\", \"-\", \"PLACE\"]}))\n",
    "    TRAIN_DATA.append((\"I want to find {}'s office\".format(name), \n",
    "              { \"heads\": [1, 1, 3, 1, 5, 7, 5, 3],  # index of token head\n",
    "                \"deps\": ['-', 'ROOT', '-', 'XCOMP', 'POSS', 'POSS', '-', 'PLACE']}))\n",
    "    TRAIN_DATA.append((\"Where is {}'s office?\".format(name), \n",
    "              { \"heads\": [1, 1, 3, 5, 3, 1, 1],  # index of token head\n",
    "                \"deps\": [\"-\", \"ROOT\", \"POSS\", \"POSS\", \"-\", \"PLACE\", \"-\"]}))\n",
    "    TRAIN_DATA.append((\"Where can I find {}'s office?\".format(name), \n",
    "              { \"heads\": [3, 3, 3, 3, 5, 7, 5, 3, 3],  # index of token head\n",
    "                \"deps\": [\"-\", \"-\", \"-\", \"ROOT\", \"POSS\", \"POSS\", \"-\", \"PLACE\", \"-\"]}))\n",
    "    TRAIN_DATA.append((\"Let me know {}'s office\".format(name), \n",
    "              { \"heads\": [2, 2, 2, 4, 6, 4, 2],  # index of token head\n",
    "                \"deps\": [\"-\", \"-\", \"ROOT\", \"POSS\", \"POSS\", \"-\", \"PLACE\"]}))    \n",
    "\n",
    "def main(model=None, output_dir=None, n_iter=15):\n",
    "    \"\"\"Load the model, set up the pipeline and train the parser.\"\"\"\n",
    "    if model is not None:\n",
    "        nlp = spacy.load(model)  # load existing spaCy model\n",
    "        print(\"Loaded model '%s'\" % model)\n",
    "    else:\n",
    "        nlp = spacy.blank(\"en\")  # create blank Language class\n",
    "        print(\"Created blank 'en' model\")\n",
    "\n",
    "    # We'll use the built-in dependency parser class, but we want to create a\n",
    "    # fresh instance â€“ just in case.\n",
    "    if \"parser\" in nlp.pipe_names:\n",
    "        nlp.remove_pipe(\"parser\")\n",
    "    parser = nlp.create_pipe(\"parser\")\n",
    "    nlp.add_pipe(parser, first=True)\n",
    "\n",
    "    for text, annotations in TRAIN_DATA:\n",
    "        for dep in annotations.get(\"deps\", []):\n",
    "            parser.add_label(dep)\n",
    "\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"parser\"]\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train parser\n",
    "        optimizer = nlp.begin_training()\n",
    "        for itn in range(n_iter):\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            losses = {}\n",
    "            # batch up the examples using spaCy's minibatch\n",
    "            batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))\n",
    "            for batch in batches:\n",
    "                texts, annotations = zip(*batch)\n",
    "                nlp.update(texts, annotations, sgd=optimizer, losses=losses)\n",
    "            print(\"Losses\", losses)\n",
    "\n",
    "    # test the trained model\n",
    "    test_model(nlp)\n",
    "\n",
    "    # save model to output directory\n",
    "    if output_dir is not None:\n",
    "        output_dir = Path(output_dir)\n",
    "        if not output_dir.exists():\n",
    "            output_dir.mkdir()\n",
    "        nlp.to_disk(output_dir)\n",
    "        print(\"Saved model to\", output_dir)\n",
    "\n",
    "        # test the saved model\n",
    "        print(\"Loading from\", output_dir)\n",
    "        nlp2 = spacy.load(output_dir)\n",
    "        test_model(nlp2)\n",
    "\n",
    "\n",
    "def test_model(nlp):\n",
    "    texts = [\n",
    "        \"find office\",\n",
    "        \"find Donna's office\",\n",
    "        \"find me Mandy Green's office\",\n",
    "        \"how can I find Donna's office\"\n",
    "    ]\n",
    "    docs = nlp.pipe(texts)\n",
    "    for doc in docs:\n",
    "        print(doc.text)\n",
    "        print([(t.text, t.dep_, t.head.text) for t in doc if t.dep_ != \"-\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model 'en_core_web_md'\n",
      "Losses {'parser': 344.5491369504598}\n",
      "Losses {'parser': 37.959823385512166}\n",
      "Losses {'parser': 27.775485268975256}\n",
      "Losses {'parser': 22.33266909289317}\n",
      "Losses {'parser': 17.95671298610493}\n",
      "Losses {'parser': 18.349029654988847}\n",
      "Losses {'parser': 7.898654287429743}\n",
      "Losses {'parser': 9.308045757443494}\n",
      "Losses {'parser': 18.447383064616435}\n",
      "Losses {'parser': 11.725313484355553}\n",
      "Losses {'parser': 9.345147627348972}\n",
      "Losses {'parser': 10.008708191997412}\n",
      "Losses {'parser': 9.99514683900395}\n",
      "Losses {'parser': 8.2670250019281}\n",
      "Losses {'parser': 8.077106823825876}\n",
      "find office\n",
      "[('find', 'ROOT', 'find'), ('office', 'PLACE', 'find')]\n",
      "find Donna's office\n",
      "[('find', 'ROOT', 'find'), ('Donna', 'POSS', 'office'), ('office', 'PLACE', 'find')]\n",
      "find me Mandy Green's office\n",
      "[('find', 'ROOT', 'find'), ('Mandy', 'POSS', 'Green'), ('Green', 'POSS', 'office'), ('office', 'PLACE', 'find')]\n",
      "how can I find Donna's office\n",
      "[('find', 'ROOT', 'find'), ('Donna', 'POSS', 'office'), ('office', 'PLACE', 'find')]\n",
      "Saved model to model\n",
      "Loading from model\n",
      "find office\n",
      "[('find', 'ROOT', 'find'), ('office', 'PLACE', 'find')]\n",
      "find Donna's office\n",
      "[('find', 'ROOT', 'find'), ('Donna', 'POSS', 'office'), ('office', 'PLACE', 'find')]\n",
      "find me Mandy Green's office\n",
      "[('find', 'ROOT', 'find'), ('Mandy', 'POSS', 'Green'), ('Green', 'POSS', 'office'), ('office', 'PLACE', 'find')]\n",
      "how can I find Donna's office\n",
      "[('find', 'ROOT', 'find'), ('Donna', 'POSS', 'office'), ('office', 'PLACE', 'find')]\n"
     ]
    }
   ],
   "source": [
    "main(\"en_core_web_md\", output_dir=\"./model\")\n",
    "#main(\"./model\", output_dir=\"./model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['parser', 'tagger', 'ner']\n",
      "[1, 1, 3, 1, 6, 4, 3]\n",
      "['-', 'ROOT', '-', '-', 'POSS', '-', 'PLACE']\n",
      "['I', 'want', 'to', 'find', 'Donna', \"'s\", 'office']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"./model\")\n",
    "# \"en_core_web_md\"\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "#text = \"let me know brian elliot's office\"\n",
    "text = \"I want to find Donna's office\"\n",
    "doc = nlp(text)\n",
    "print([token.head.i for token in doc])\n",
    "print([token.dep_ for token in doc])\n",
    "# print([token.pos_ for token in doc])\n",
    "print([token.text for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tagger', 'parser', 'ner']\n",
      "[1, 1, 3, 1, 5, 7, 5, 3]\n",
      "['nsubj', 'ROOT', 'aux', 'xcomp', 'compound', 'poss', 'case', 'dobj']\n",
      "['I', 'want', 'to', 'find', 'mandy', 'green', \"'s\", 'office']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "#text = \"let me know brian elliot's office\"\n",
    "text = \"I want to find mandy green's office\"\n",
    "doc = nlp(text)\n",
    "print([token.head.i for token in doc])\n",
    "print([token.dep_ for token in doc])\n",
    "# print([token.pos_ for token in doc])\n",
    "print([token.text for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I want to find Donna's office\n",
      "PERSON Donna\n",
      "PERSON Donna\n",
      "defaultdict(<class 'list'>, {'PERSON': 'Donna'})\n"
     ]
    }
   ],
   "source": [
    "for sent in doc.sents:\n",
    "    print(sent)\n",
    "    \n",
    "entities = defaultdict(list)\n",
    "for ent in doc.ents:\n",
    "    entities[ent.label_] = ent.text\n",
    "\n",
    "print(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] [4] [6]\n",
      "want\n",
      "FIND_OFFICE_LOC\n",
      "defaultdict(<class 'list'>, {'PLACE': ['office'], 'POSS': ['teacher']})\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "deps = [token.dep_ for token in doc]\n",
    "root = [i for i, dep in enumerate(deps) if dep == 'ROOT']\n",
    "xcomp = [i for i, dep in enumerate(deps) if dep == 'XCOMP']\n",
    "poss = [i for i, dep in enumerate(deps) if dep == 'POSS']\n",
    "place = [i for i, dep in enumerate(deps) if dep == 'PLACE']\n",
    "print(root, poss, place)\n",
    "entities = defaultdict(list)\n",
    "entities_to_have = [\"POSS\", \"PLACE\"]\n",
    "intent_find_words = [\"find\", \"search\", \"know\", \"wanna\", \"want\"]\n",
    "entity_office_words = [\"office\", \"room\", \"place\"]\n",
    "\n",
    "intent = \"UNK\"\n",
    "if len(root) > 1 or len(xcomp) > 1 or len(poss) > 2 or len(place) > 1:\n",
    "    intent = \"COMPLEX\"\n",
    "elif str(doc[root[0]]).lower() in entity_office_words and len(place) == 1:\n",
    "    intent = \"COMPLEX\"\n",
    "elif len(poss) == 2 and (poss[0] - poss[1])**2 > 1:\n",
    "    intent = \"COMPLEX\"\n",
    "else:\n",
    "    temp = []\n",
    "    for i, t in enumerate(doc):\n",
    "        if t.dep_ in entities_to_have and (t.head.i == root[0] or t.head.i == xcomp[0]):\n",
    "            entities[t.dep_].append(str(t))\n",
    "            temp.append(i)\n",
    "\n",
    "    while(len(temp)):\n",
    "        temp_copy = temp.copy()\n",
    "        temp = []\n",
    "        for i, t in enumerate(doc):\n",
    "            if t.dep_ in entities_to_have and t.head.i in temp_copy:\n",
    "                entities[t.dep_].append(str(t))\n",
    "                temp.append(i)\n",
    "\n",
    "    root_string = str(doc[root[0]]).lower()\n",
    "    xcomp_string = str(doc[xcomp[0]]).lower()\n",
    "\n",
    "    if root_string in entity_office_words:\n",
    "        intent = \"FIND_OFFICE_LOC\"\n",
    "    elif root_string in intent_find_words or xcomp_string in intent_find_words:\n",
    "        found = False\n",
    "        for ent_A in entity_office_words:\n",
    "            for ent_B in entities.get(\"PLACE\",[]):\n",
    "                if ent_A == ent_B.lower():\n",
    "                    found = True\n",
    "                    break\n",
    "        if found:\n",
    "            intent = \"FIND_OFFICE_LOC\"\n",
    "\n",
    "print(root_string)\n",
    "print(intent)\n",
    "print(entities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
